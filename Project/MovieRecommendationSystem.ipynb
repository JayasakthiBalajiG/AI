{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8bdbf1-2e4e-48ab-b536-df7bd942dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests  # For downloading dataset\n",
    "import pandas as pd  # For handling data\n",
    "import torch  # For model operations\n",
    "import transformers  # For using a pre-trained model from HuggingFace\n",
    "import numpy as np\n",
    "import sklearn.metrics  # For evaluating model performance\n",
    "import zipfile\n",
    "import io\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. DATA INGESTION COMPONENT\n",
    "class DataIngestion:\n",
    "    def __init__(self, dataset_url):\n",
    "        self.dataset_url = dataset_url\n",
    "    \n",
    "    def download_data(self):\n",
    "        # Download dataset from online source\n",
    "        response = requests.get(self.dataset_url)\n",
    "        # Unzip and load the datasets into pandas\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Load the 'u.data' file (user ratings)\n",
    "            with z.open('ml-100k/u.data') as f:\n",
    "                ratings = pd.read_csv(f, sep='\\t', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "            # Load the 'u.item' file (movie information)\n",
    "            with z.open('ml-100k/u.item') as f:\n",
    "                items = pd.read_csv(f, sep='|', header=None, encoding='latin-1', names=['item_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])\n",
    "\n",
    "            # Load the 'u.user' file (user information)\n",
    "            with z.open('ml-100k/u.user') as f:\n",
    "                users = pd.read_csv(f, sep='|', header=None, encoding='latin-1', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n",
    "\n",
    "            merged_data = pd.merge(ratings, items, on='item_id')\n",
    "            # Merge the result with users\n",
    "            data = pd.merge(merged_data, users, on='user_id')\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606c6b17-7d7c-4221-8973-4ca09ad5748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATA PREPROCESSING COMPONENT\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def clean_data(self):\n",
    "        # Drop unnecessary columns\n",
    "        relevant_columns = ['title', 'age', 'gender', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "        cleaned_data = self.data[relevant_columns]\n",
    "        \n",
    "        # Remove rows with any null values\n",
    "        cleaned_data = cleaned_data.dropna()\n",
    "        # Encode gender: 'F' -> 0, 'M' -> 1\n",
    "        cleaned_data['gender'] = cleaned_data['gender'].map({'F': 0, 'M': 1})\n",
    "        \n",
    "        # Encode title\n",
    "        # Initialize LabelEncoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # Apply label encoding to the 'title' column\n",
    "        cleaned_data['title'] = label_encoder.fit_transform(cleaned_data['title'])\n",
    "\n",
    "        cleaned_data = cleaned_data[(cleaned_data['title'] >= 0) & (cleaned_data['title'] <= 100)]\n",
    "   \n",
    "        return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb0504c-930f-4dd6-9870-9f15eb5ff726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MODEL COMPONENT\n",
    "class PretrainedModel:\n",
    "    def __init__(self, model_name):\n",
    "        with open(model_name, 'rb') as f:\n",
    "            self.model = joblib.load(f)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = inputs.drop(columns=['title'])\n",
    "        return self.model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad9c846-ec63-4b47-95a3-a56ad30c5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SYSTEM COMPONENT - PIPELINE\n",
    "class MLPipeline:\n",
    "    def __init__(self, dataset_url, model_name):\n",
    "        self.data_ingestion = DataIngestion(dataset_url)\n",
    "        self.preprocessor = None\n",
    "        self.model = PretrainedModel(model_name)\n",
    "        self.data = None\n",
    "    \n",
    "    def build_pipeline(self):\n",
    "        # Step 1: Download and load data\n",
    "        self.data = self.data_ingestion.download_data()\n",
    "        \n",
    "        # Step 2: Preprocess data\n",
    "        self.preprocessor = DataPreprocessor(self.data)\n",
    "        cleaned_data = self.preprocessor.clean_data()\n",
    "        \n",
    "        # Step 3: Pass data through the model for prediction\n",
    "        predictions = self.model.predict(cleaned_data)\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1bcf1d6-0ad4-4587-9d28-4c974cfa5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. FEEDBACK COMPONENT\n",
    "class FeedbackLoop:\n",
    "    def __init__(self, predictions, true_labels, dataset_url):\n",
    "        self.predictions = predictions\n",
    "        self.true_labels = true_labels\n",
    "        self.data_ingestion = DataIngestion(dataset_url)\n",
    "    \n",
    "    def simulate_user_feedback(self):\n",
    "        # actual accuracy of the model\n",
    "        accuracy = accuracy_score(self.true_labels, self.predictions)\n",
    "\n",
    "        # Step 1: Download and load data for training\n",
    "        self.data = self.data_ingestion.download_data()\n",
    "        \n",
    "        # Step 2: Preprocess data\n",
    "        self.preprocessor = DataPreprocessor(self.data)\n",
    "        df = self.preprocessor.clean_data()\n",
    "        \n",
    "        # Define feature columns and target variable\n",
    "        X = df.drop(columns=['title'])  # Features (excluding 'title')\n",
    "        y = df['title']  # Target variable (title)\n",
    "        \n",
    "        # Split the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Initialize and train the Gradient Boosting Classifier\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        accuracy_trained = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        feedback = \"improve model\" if accuracy < accuracy_trained else \"model is performing well\"\n",
    "        return feedback\n",
    "    \n",
    "    def adjust_model_based_on_feedback(self, feedback):\n",
    "        # Simulate adjustments in the system based on user feedback\n",
    "        if feedback == 'improve model':\n",
    "            print(\"Feedback received: Retraining model is performing better, upload retrained model to huggingface\")\n",
    "            #in this section you should push your retrained model to huggingface so you will have updated model and can use it in the new prediction\n",
    "        else:\n",
    "            print(\"Feedback received: No major changes needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e48ecd4-7d73-40f1-aef1-348d4dbdbd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_labels(dataset_url):\n",
    "    data_ingestion = DataIngestion(dataset_url)\n",
    "    data = data_ingestion.download_data()\n",
    "    \n",
    "    preprocessor = DataPreprocessor(data)\n",
    "    cleaned_data = preprocessor.clean_data()\n",
    "    \n",
    "    # Extract true labels from the 'title' column\n",
    "    true_labels = np.array(cleaned_data['title'])\n",
    "    \n",
    "    return true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2ebd2b3-4a02-4551-8776-36a950af803c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback received: No major changes needed\n"
     ]
    }
   ],
   "source": [
    "# 6. SYSTEM EXECUTION\n",
    "def main():\n",
    "    # Initialize pipeline with MovieLens 100K dataset and HuggingFace model\n",
    "    dataset_url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "    model_name = hf_hub_download(repo_id='bnamazci/gradient-boosting-model2', filename='gradient_boosting_model.joblib')\n",
    "    \n",
    "    # Step 1: Build and execute the pipeline\n",
    "    pipeline = MLPipeline(dataset_url, model_name)\n",
    "    predictions = pipeline.build_pipeline()\n",
    "\n",
    "    true_labels = get_true_labels(dataset_url)\n",
    "    feedback_loop = FeedbackLoop(predictions, true_labels, dataset_url)\n",
    "    \n",
    "    # Step 3: Handle feedback and adjust system\n",
    "    feedback = feedback_loop.simulate_user_feedback()\n",
    "    feedback_loop.adjust_model_based_on_feedback(feedback)\n",
    "\n",
    "# Run the main system\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf55cf0-188f-477e-9b0c-38a8f8556c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
